<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Subsampling for Class Imbalances • recipes</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootswatch/3.3.7/united/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.7.1/clipboard.min.js" integrity="sha384-cV+rhyOuRHc9Ub/91rihWcGmMmCXDeksTtCihMupQHSsi8GIIRDG0ThDc3HGQFJ3" crossorigin="anonymous"></script><!-- sticky kit --><script src="https://cdnjs.cloudflare.com/ajax/libs/sticky-kit/1.1.3/sticky-kit.min.js" integrity="sha256-c4Rlo1ZozqTPE2RLuvbusY3+SU1pQaJC0TjuhygMipw=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../../pkgdown.css" rel="stylesheet">
<script src="../../pkgdown.js"></script><link href="../../extra.css" rel="stylesheet">
<meta property="og:title" content="Subsampling for Class Imbalances">
<meta property="og:description" content="">
<meta name="twitter:card" content="summary">
<meta name="robots" content="noindex">
<!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../../index.html">recipes</a>
        <span class="label label-danger" data-toggle="tooltip" data-placement="bottom" title="In-development package">0.1.3.9000</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../../articles/Simple_Example.html">Simple Example</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../../articles/Selecting_Variables.html">Selecting Variables</a>
    </li>
    <li>
      <a href="../../articles/Custom_Steps.html">Custom Steps</a>
    </li>
    <li>
      <a href="../../articles/Ordering.html">The Order of Steps</a>
    </li>
    <li>
      <a href="../../articles/Dummies.html">Dummy Variables and Interactions</a>
    </li>
    <li>
      <a href="../../articles/Skipping.html">On Skipping Steps</a>
    </li>
    <li>
      <a href="../../articles/articles/Subsampling.html">Subsampling for Class Imbalances</a>
    </li>
    <li>
      <a href="../../articles/articles/Multivariate_PLS.html">Multivariate Analysis using Partial Least Squares</a>
    </li>
  </ul>
</li>
<li>
  <a href="../../reference/index.html">Reference</a>
</li>
<li>
  <a href="../../news/index.html">News</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/tidymodels/recipes">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1>Subsampling for Class Imbalances</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/tidymodels/recipes/blob/master/vignettes/articles/Subsampling.Rmd"><code>vignettes/articles/Subsampling.Rmd</code></a></small>
      <div class="hidden name"><code>Subsampling.Rmd</code></div>

    </div>

    
    
<p>Subsampling can be a helpful approach to dealing will classification data where one or more classes occur very infrequently. Often, most models will overfit to the majority class and produce very good statistics for the class containing the frequently occurring classes while the minority classes have poor performance.</p>
<p>Consider a two-class problem where the first class has a very low rate of occurrence. The <a href="https://topepo.github.io/caret/"><code>caret</code></a> package has a function that can simulate such data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(caret)

<span class="kw">set.seed</span>(<span class="dv">244</span>)
imbal_data &lt;-<span class="st"> </span><span class="kw"><a href="http://www.rdocumentation.org/packages/caret/topics/twoClassSim">twoClassSim</a></span>(<span class="dv">1000</span>, <span class="dt">intercept =</span> <span class="dv">10</span>)
<span class="kw">table</span>(imbal_data<span class="op">$</span>Class)
<span class="co">#&gt; </span>
<span class="co">#&gt; Class1 Class2 </span>
<span class="co">#&gt;     47    953</span></code></pre></div>
<p>If “Class1” is the event of interest, it is very likely that a classification model would be able to achieve very good <em>specificity</em> since almost all of the data are the second class. <em>Sensitivity</em> will often be poor since the models will optimize accuracy (or other loss functions) by predicting everything to be the majority class.</p>
<p>When there are two classes, the results is that the default probability cutoff of 50% is inappropriate; a different cutoff that is more extreme might be able to achieve good performance.</p>
<p>One way to alleviate this issue is to <em>subsample</em> the data. There are a number of ways to do this but the most simple one is to <em>sample down</em> the majority class data until it occurs with the same frequency as the minority class. While counterintuitive, throwing out a large percentage of the data can be effective at producing a results. In some cases, this means that the overall performance of the model is better (e.g. improved area under the ROC curve). However, subsampling almost always produces models that are <em>better calibrated</em>, meaning that the distributions of the class probabilities are model well behaved. As a result, the default 50% cutoff is much model likely to produce better sensitivity and specificity values than they would otherwise.</p>
<p>To demonstrate this, <code>step_downsample</code> will be used in a recipe for the simulated data. In terms of workflow:</p>
<ul>
<li>It is extremely important that subsampling occurs <em>inside of resampling</em>. Otherwise, the resampling process can produce <a href="https://topepo.github.io/caret/subsampling-for-class-imbalances.html#resampling">poor estimates of model performance</a>.</li>
<li>The subsampling process should only be applied to the analysis set. The assessment set should reflect the event rates seen “in the wild” and, for this reason, the <code>skip</code> argument to <code>step_downsample</code> is defaulted to <code>TRUE</code>.</li>
</ul>
<p>Here is a simple recipe:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(recipes)
imbal_rec &lt;-<span class="st"> </span><span class="kw"><a href="../../reference/recipe.html">recipe</a></span>(Class <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> imbal_data) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="../../reference/step_downsample.html">step_downsample</a></span>(Class)</code></pre></div>
<p>Basic cross-validation is used to resample the model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(rsample)
<span class="kw">set.seed</span>(<span class="dv">5732</span>)
cv_folds &lt;-<span class="st"> </span><span class="kw"><a href="http://www.rdocumentation.org/packages/rsample/topics/vfold_cv">vfold_cv</a></span>(imbal_data, <span class="dt">strata =</span> <span class="st">"Class"</span>, <span class="dt">repeats =</span> <span class="dv">5</span>)</code></pre></div>
<p>An additional column is added to the data that contains the trained recipes for each resample:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(purrr)
cv_folds &lt;-<span class="st"> </span>cv_folds <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">recipes =</span> 
           <span class="kw"><a href="http://www.rdocumentation.org/packages/purrr/topics/map">map</a></span>(cv_folds<span class="op">$</span>splits, prepper, <span class="dt">recipe =</span> imbal_rec, <span class="dt">retain =</span> <span class="ot">TRUE</span>))
cv_folds<span class="op">$</span>recipes[[<span class="dv">1</span>]]
<span class="co">#&gt; Data Recipe</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Inputs:</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;       role #variables</span>
<span class="co">#&gt;    outcome          1</span>
<span class="co">#&gt;  predictor         15</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Training data contained 900 data points and no missing data.</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Operations:</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Down-sampling based on Class [trained]</span></code></pre></div>
<p>The model that will be used to demonstrate subsampling is <a href="https://en.wikipedia.org/wiki/Quadratic_classifier#Quadratic_discriminant_analysis">quadratic discriminant analysis</a> via the <code>MASS</code> package. A function will be used to train the model and to produce class probabilities as well as hard class predictions using the default 50% cutoff. When a recipe is passed to the function, down-sampling will be applied. If no recipe is given, the data are used to fit the model as-is:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(MASS)
<span class="kw">library</span>(dplyr)

assess_res &lt;-<span class="st"> </span><span class="cf">function</span>(split, <span class="dt">rec =</span> <span class="ot">NULL</span>, ...) {
  <span class="cf">if</span> (<span class="op">!</span><span class="kw">is.null</span>(rec))
    mod_data &lt;-<span class="st"> </span><span class="kw"><a href="../../reference/juice.html">juice</a></span>(rec)
  <span class="cf">else</span>
    mod_data &lt;-<span class="st"> </span><span class="kw"><a href="http://www.rdocumentation.org/packages/rsample/topics/as.data.frame.rsplit">analysis</a></span>(split)
  
  mod_fit &lt;-<span class="st"> </span><span class="kw"><a href="http://www.rdocumentation.org/packages/MASS/topics/qda">qda</a></span>(Class <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> mod_data)
  
  <span class="cf">if</span> (<span class="op">!</span><span class="kw">is.null</span>(rec))
    eval_data &lt;-<span class="st"> </span><span class="kw"><a href="../../reference/bake.html">bake</a></span>(rec, <span class="kw"><a href="http://www.rdocumentation.org/packages/rsample/topics/as.data.frame.rsplit">assessment</a></span>(split))
  <span class="cf">else</span>
    eval_data &lt;-<span class="st"> </span><span class="kw"><a href="http://www.rdocumentation.org/packages/rsample/topics/as.data.frame.rsplit">assessment</a></span>(split)
  
  eval_data &lt;-<span class="st"> </span>eval_data 
  predictions &lt;-<span class="st"> </span><span class="kw">predict</span>(mod_fit, eval_data)
  eval_data <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw"><a href="http://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span>(
      <span class="dt">pred =</span> predictions<span class="op">$</span>class,
      <span class="dt">prob =</span> predictions<span class="op">$</span>posterior[,<span class="dv">1</span>]
    ) <span class="op">%&gt;%</span>
<span class="st">    </span>dplyr<span class="op">::</span><span class="kw"><a href="http://dplyr.tidyverse.org/reference/select.html">select</a></span>(Class, pred, prob)
}</code></pre></div>
<p>For example:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># No subsampling</span>
<span class="kw">assess_res</span>(cv_folds<span class="op">$</span>splits[[<span class="dv">1</span>]]) <span class="op">%&gt;%</span><span class="st"> </span>head
<span class="co">#&gt;    Class   pred     prob</span>
<span class="co">#&gt; 1 Class2 Class2 2.36e-07</span>
<span class="co">#&gt; 2 Class2 Class2 4.69e-06</span>
<span class="co">#&gt; 3 Class2 Class2 2.21e-04</span>
<span class="co">#&gt; 4 Class2 Class2 1.20e-03</span>
<span class="co">#&gt; 5 Class1 Class2 2.78e-01</span>
<span class="co">#&gt; 6 Class2 Class2 6.13e-11</span>

<span class="co"># With downsampling:</span>
<span class="kw">assess_res</span>(cv_folds<span class="op">$</span>splits[[<span class="dv">1</span>]], cv_folds<span class="op">$</span>recipes[[<span class="dv">1</span>]]) <span class="op">%&gt;%</span><span class="st"> </span>head
<span class="co">#&gt; # A tibble: 6 x 3</span>
<span class="co">#&gt;   Class  pred           prob</span>
<span class="co">#&gt;   &lt;fct&gt;  &lt;fct&gt;         &lt;dbl&gt;</span>
<span class="co">#&gt; 1 Class2 Class2 0.0933      </span>
<span class="co">#&gt; 2 Class2 Class2 0.000340    </span>
<span class="co">#&gt; 3 Class2 Class2 0.0255      </span>
<span class="co">#&gt; 4 Class2 Class2 0.00472     </span>
<span class="co">#&gt; 5 Class1 Class1 1.000       </span>
<span class="co">#&gt; 6 Class2 Class2 0.0000000371</span></code></pre></div>
<p>To measure model effectiveness, two metrics are used:</p>
<ul>
<li>The area under the <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic">ROC curve</a> is an overall assessment of performance across <em>all</em> cutoffs. Values near one indicate very good results while values near 0.05 would imply that the model is very poor.</li>
<li>The <em>J</em> index (a.k.a. <a href="https://en.wikipedia.org/wiki/Youden%27s_J_statistic">Youden’s <em>J</em></a> statistic) is <code>sensitivity + specificity - 1</code>. Values near one are once again best.</li>
</ul>
<p>If a model is poorly calibrated, the ROC curve value might not show diminished performance. However, the <em>J</em> index would be lower for models with pathological distributions for the class probabilities. The <code>yardstick</code> package will be used to compute these metrics.</p>
<p>Now, we train the models and generate the predictions. These are stored in list columns where each list element is a data frame of the predictions on the assessment data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cv_folds &lt;-<span class="st"> </span>cv_folds <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="http://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span>(
    <span class="dt">sampled_pred =</span> <span class="kw"><a href="http://www.rdocumentation.org/packages/purrr/topics/map2">map2</a></span>(cv_folds<span class="op">$</span>splits, cv_folds<span class="op">$</span>recipes, assess_res),
    <span class="dt">normal_pred =</span> <span class="kw"><a href="http://www.rdocumentation.org/packages/purrr/topics/map">map</a></span>(cv_folds<span class="op">$</span>splits, assess_res)
  )
cv_folds
<span class="co">#&gt; #  10-fold cross-validation repeated 5 times using stratification </span>
<span class="co">#&gt; # A tibble: 50 x 6</span>
<span class="co">#&gt;    splits       id      id2    recipes      sampled_pred       normal_pred</span>
<span class="co">#&gt;  * &lt;list&gt;       &lt;chr&gt;   &lt;chr&gt;  &lt;list&gt;       &lt;list&gt;             &lt;list&gt;     </span>
<span class="co">#&gt;  1 &lt;S3: rsplit&gt; Repeat1 Fold01 &lt;S3: recipe&gt; &lt;tibble [100 × 3]&gt; &lt;data.fram…</span>
<span class="co">#&gt;  2 &lt;S3: rsplit&gt; Repeat1 Fold02 &lt;S3: recipe&gt; &lt;tibble [100 × 3]&gt; &lt;data.fram…</span>
<span class="co">#&gt;  3 &lt;S3: rsplit&gt; Repeat1 Fold03 &lt;S3: recipe&gt; &lt;tibble [100 × 3]&gt; &lt;data.fram…</span>
<span class="co">#&gt;  4 &lt;S3: rsplit&gt; Repeat1 Fold04 &lt;S3: recipe&gt; &lt;tibble [100 × 3]&gt; &lt;data.fram…</span>
<span class="co">#&gt;  5 &lt;S3: rsplit&gt; Repeat1 Fold05 &lt;S3: recipe&gt; &lt;tibble [100 × 3]&gt; &lt;data.fram…</span>
<span class="co">#&gt;  6 &lt;S3: rsplit&gt; Repeat1 Fold06 &lt;S3: recipe&gt; &lt;tibble [100 × 3]&gt; &lt;data.fram…</span>
<span class="co">#&gt;  7 &lt;S3: rsplit&gt; Repeat1 Fold07 &lt;S3: recipe&gt; &lt;tibble [100 × 3]&gt; &lt;data.fram…</span>
<span class="co">#&gt;  8 &lt;S3: rsplit&gt; Repeat1 Fold08 &lt;S3: recipe&gt; &lt;tibble [100 × 3]&gt; &lt;data.fram…</span>
<span class="co">#&gt;  9 &lt;S3: rsplit&gt; Repeat1 Fold09 &lt;S3: recipe&gt; &lt;tibble [100 × 3]&gt; &lt;data.fram…</span>
<span class="co">#&gt; 10 &lt;S3: rsplit&gt; Repeat1 Fold10 &lt;S3: recipe&gt; &lt;tibble [100 × 3]&gt; &lt;data.fram…</span>
<span class="co">#&gt; # ... with 40 more rows</span></code></pre></div>
<p>Now, the performance metrics are computed:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(yardstick)
cv_folds &lt;-<span class="st"> </span>cv_folds <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="http://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span>(
    <span class="dt">sampled_roc =</span> <span class="kw"><a href="http://www.rdocumentation.org/packages/purrr/topics/map">map_dbl</a></span>(cv_folds<span class="op">$</span>sampled_pred, roc_auc, <span class="dt">truth =</span> <span class="st">"Class"</span>, <span class="dt">estimate =</span> <span class="st">"prob"</span>),
    <span class="dt">normal_roc =</span>  <span class="kw"><a href="http://www.rdocumentation.org/packages/purrr/topics/map">map_dbl</a></span>(cv_folds<span class="op">$</span>normal_pred,  roc_auc, <span class="dt">truth =</span> <span class="st">"Class"</span>, <span class="dt">estimate =</span> <span class="st">"prob"</span>),  
    <span class="dt">sampled_J =</span>   <span class="kw"><a href="http://www.rdocumentation.org/packages/purrr/topics/map">map_dbl</a></span>(cv_folds<span class="op">$</span>sampled_pred, j_index, <span class="dt">truth =</span> <span class="st">"Class"</span>, <span class="dt">estimate =</span> <span class="st">"pred"</span>),
    <span class="dt">normal_J =</span>    <span class="kw"><a href="http://www.rdocumentation.org/packages/purrr/topics/map">map_dbl</a></span>(cv_folds<span class="op">$</span>normal_pred,  j_index, <span class="dt">truth =</span> <span class="st">"Class"</span>, <span class="dt">estimate =</span> <span class="st">"pred"</span>)       
  )</code></pre></div>
<p>What do the ROC values look like? A <a href="https://en.wikipedia.org/wiki/Bland%E2%80%93Altman_plot">Bland-Altman plot</a> can be used to show the differences in the results over the range of results:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ggplot2)

<span class="kw"><a href="http://www.rdocumentation.org/packages/ggplot2/topics/ggplot">ggplot</a></span>(cv_folds, 
       <span class="kw"><a href="http://www.rdocumentation.org/packages/ggplot2/topics/aes">aes</a></span>(<span class="dt">x =</span> (sampled_roc <span class="op">+</span><span class="st"> </span>normal_roc)<span class="op">/</span><span class="dv">2</span>, 
           <span class="dt">y =</span> sampled_roc <span class="op">-</span><span class="st"> </span>normal_roc)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw"><a href="http://www.rdocumentation.org/packages/ggplot2/topics/geom_point">geom_point</a></span>() <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw"><a href="http://www.rdocumentation.org/packages/ggplot2/topics/geom_abline">geom_hline</a></span>(<span class="dt">yintercept =</span> <span class="dv">0</span>, <span class="dt">col =</span> <span class="st">"green"</span>)</code></pre></div>
<p><img src="Subsampling_files/figure-html/bland-altman-roc-1.png" width="768"></p>
<p>There doesn’t appear that subsampling had much of an effect on this metric. The average difference is -0.011, which is fairly small.</p>
<p>For the <em>J</em> statistic, the results show a different story:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="http://www.rdocumentation.org/packages/ggplot2/topics/ggplot">ggplot</a></span>(cv_folds, 
       <span class="kw"><a href="http://www.rdocumentation.org/packages/ggplot2/topics/aes">aes</a></span>(<span class="dt">x =</span> (sampled_J <span class="op">+</span><span class="st"> </span>normal_J)<span class="op">/</span><span class="dv">2</span>, 
           <span class="dt">y =</span> sampled_J <span class="op">-</span><span class="st"> </span>normal_J)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw"><a href="http://www.rdocumentation.org/packages/ggplot2/topics/geom_point">geom_point</a></span>() <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw"><a href="http://www.rdocumentation.org/packages/ggplot2/topics/geom_abline">geom_hline</a></span>(<span class="dt">yintercept =</span> <span class="dv">0</span>, <span class="dt">col =</span> <span class="st">"green"</span>)</code></pre></div>
<p><img src="Subsampling_files/figure-html/bland-altman-j-1.png" width="768"></p>
<p>Almost all of the differences area greater than zero. We can use <code>tidyposterior</code> to do a more formal analysis:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyposterior)

<span class="co"># Remove all columns except the resample info and the J indices,</span>
<span class="co"># then fit the Bayesian model</span>
j_mod &lt;-<span class="st"> </span>cv_folds <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span>dplyr<span class="op">::</span><span class="kw"><a href="http://dplyr.tidyverse.org/reference/select.html">select</a></span>(<span class="op">-</span>recipes, <span class="op">-</span><span class="kw"><a href="http://dplyr.tidyverse.org/reference/reexports.html">matches</a></span>(<span class="st">"pred$"</span>), <span class="op">-</span><span class="kw"><a href="http://dplyr.tidyverse.org/reference/reexports.html">matches</a></span>(<span class="st">"roc$"</span>)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw"><a href="http://www.rdocumentation.org/packages/tidyposterior/topics/perf_mod">perf_mod</a></span>(<span class="dt">seed =</span> <span class="dv">62378</span>, <span class="dt">iter =</span> <span class="dv">5000</span>)
<span class="co">#&gt; </span>
<span class="co">#&gt; SAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Gradient evaluation took 0.000153 seconds</span>
<span class="co">#&gt; 1000 transitions using 10 leapfrog steps per transition would take 1.53 seconds.</span>
<span class="co">#&gt; Adjust your expectations accordingly!</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Iteration:    1 / 5000 [  0%]  (Warmup)</span>
<span class="co">#&gt; Iteration:  500 / 5000 [ 10%]  (Warmup)</span>
<span class="co">#&gt; Iteration: 1000 / 5000 [ 20%]  (Warmup)</span>
<span class="co">#&gt; Iteration: 1500 / 5000 [ 30%]  (Warmup)</span>
<span class="co">#&gt; Iteration: 2000 / 5000 [ 40%]  (Warmup)</span>
<span class="co">#&gt; Iteration: 2500 / 5000 [ 50%]  (Warmup)</span>
<span class="co">#&gt; Iteration: 2501 / 5000 [ 50%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 3000 / 5000 [ 60%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 3500 / 5000 [ 70%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 4000 / 5000 [ 80%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 4500 / 5000 [ 90%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 5000 / 5000 [100%]  (Sampling)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;  Elapsed Time: 1.91028 seconds (Warm-up)</span>
<span class="co">#&gt;                1.52779 seconds (Sampling)</span>
<span class="co">#&gt;                3.43807 seconds (Total)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; SAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Gradient evaluation took 6.6e-05 seconds</span>
<span class="co">#&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.66 seconds.</span>
<span class="co">#&gt; Adjust your expectations accordingly!</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Iteration:    1 / 5000 [  0%]  (Warmup)</span>
<span class="co">#&gt; Iteration:  500 / 5000 [ 10%]  (Warmup)</span>
<span class="co">#&gt; Iteration: 1000 / 5000 [ 20%]  (Warmup)</span>
<span class="co">#&gt; Iteration: 1500 / 5000 [ 30%]  (Warmup)</span>
<span class="co">#&gt; Iteration: 2000 / 5000 [ 40%]  (Warmup)</span>
<span class="co">#&gt; Iteration: 2500 / 5000 [ 50%]  (Warmup)</span>
<span class="co">#&gt; Iteration: 2501 / 5000 [ 50%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 3000 / 5000 [ 60%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 3500 / 5000 [ 70%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 4000 / 5000 [ 80%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 4500 / 5000 [ 90%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 5000 / 5000 [100%]  (Sampling)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;  Elapsed Time: 1.7676 seconds (Warm-up)</span>
<span class="co">#&gt;                1.52713 seconds (Sampling)</span>
<span class="co">#&gt;                3.29473 seconds (Total)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; SAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Gradient evaluation took 4.2e-05 seconds</span>
<span class="co">#&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.42 seconds.</span>
<span class="co">#&gt; Adjust your expectations accordingly!</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Iteration:    1 / 5000 [  0%]  (Warmup)</span>
<span class="co">#&gt; Iteration:  500 / 5000 [ 10%]  (Warmup)</span>
<span class="co">#&gt; Iteration: 1000 / 5000 [ 20%]  (Warmup)</span>
<span class="co">#&gt; Iteration: 1500 / 5000 [ 30%]  (Warmup)</span>
<span class="co">#&gt; Iteration: 2000 / 5000 [ 40%]  (Warmup)</span>
<span class="co">#&gt; Iteration: 2500 / 5000 [ 50%]  (Warmup)</span>
<span class="co">#&gt; Iteration: 2501 / 5000 [ 50%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 3000 / 5000 [ 60%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 3500 / 5000 [ 70%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 4000 / 5000 [ 80%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 4500 / 5000 [ 90%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 5000 / 5000 [100%]  (Sampling)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;  Elapsed Time: 1.89605 seconds (Warm-up)</span>
<span class="co">#&gt;                1.54538 seconds (Sampling)</span>
<span class="co">#&gt;                3.44143 seconds (Total)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; SAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Gradient evaluation took 6.6e-05 seconds</span>
<span class="co">#&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.66 seconds.</span>
<span class="co">#&gt; Adjust your expectations accordingly!</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Iteration:    1 / 5000 [  0%]  (Warmup)</span>
<span class="co">#&gt; Iteration:  500 / 5000 [ 10%]  (Warmup)</span>
<span class="co">#&gt; Iteration: 1000 / 5000 [ 20%]  (Warmup)</span>
<span class="co">#&gt; Iteration: 1500 / 5000 [ 30%]  (Warmup)</span>
<span class="co">#&gt; Iteration: 2000 / 5000 [ 40%]  (Warmup)</span>
<span class="co">#&gt; Iteration: 2500 / 5000 [ 50%]  (Warmup)</span>
<span class="co">#&gt; Iteration: 2501 / 5000 [ 50%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 3000 / 5000 [ 60%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 3500 / 5000 [ 70%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 4000 / 5000 [ 80%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 4500 / 5000 [ 90%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 5000 / 5000 [100%]  (Sampling)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;  Elapsed Time: 1.77564 seconds (Warm-up)</span>
<span class="co">#&gt;                1.6118 seconds (Sampling)</span>
<span class="co">#&gt;                3.38744 seconds (Total)</span></code></pre></div>
<p>A simple plot of the posterior distributions of the <em>J</em> indices for each model shows that there is a real difference; subsampling the data prior to modeling produced better calibrated models:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="http://www.rdocumentation.org/packages/ggplot2/topics/ggplot">ggplot</a></span>(<span class="kw">tidy</span>(j_mod, <span class="dt">seed =</span> <span class="dv">234</span>))</code></pre></div>
<p><img src="Subsampling_files/figure-html/post-plot-1.png" width="768"></p>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Max Kuhn, <a href="http://hadley.nz">Hadley Wickham</a>.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://pkgdown.r-lib.org/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  

  </body>
</html>
